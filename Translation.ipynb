{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nllb_langs = {\n",
    "    \"af\": \"afr_Latn\",\n",
    "    \"ak\": \"aka_Latn\",\n",
    "    \"am\": \"amh_Ethi\",\n",
    "    \"ar\": \"arb_Arab\",\n",
    "    \"as\": \"asm_Beng\",\n",
    "    \"ay\": \"ayr_Latn\",\n",
    "    \"az\": \"azj_Latn\",\n",
    "    \"bm\": \"bam_Latn\",\n",
    "    \"be\": \"bel_Cyrl\",\n",
    "    \"bn\": \"ben_Beng\",\n",
    "    \"bho\": \"bho_Deva\",\n",
    "    \"bs\": \"bos_Latn\",\n",
    "    \"bg\": \"bul_Cyrl\",\n",
    "    \"ca\": \"cat_Latn\",\n",
    "    \"ceb\": \"ceb_Latn\",\n",
    "    \"cs\": \"ces_Latn\",\n",
    "    \"ckb\": \"ckb_Arab\",\n",
    "    \"tt\": \"crh_Latn\",\n",
    "    \"cy\": \"cym_Latn\",\n",
    "    \"da\": \"dan_Latn\",\n",
    "    \"de\": \"deu_Latn\",\n",
    "    \"el\": \"ell_Grek\",\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"eo\": \"epo_Latn\",\n",
    "    \"et\": \"est_Latn\",\n",
    "    \"eu\": \"eus_Latn\",\n",
    "    \"ee\": \"ewe_Latn\",\n",
    "    \"fa\": \"pes_Arab\",\n",
    "    \"fi\": \"fin_Latn\",\n",
    "    \"fr\": \"fra_Latn\",\n",
    "    \"gd\": \"gla_Latn\",\n",
    "    \"ga\": \"gle_Latn\",\n",
    "    \"gl\": \"glg_Latn\",\n",
    "    \"gn\": \"grn_Latn\",\n",
    "    \"gu\": \"guj_Gujr\",\n",
    "    \"ht\": \"hat_Latn\",\n",
    "    \"ha\": \"hau_Latn\",\n",
    "    \"he\": \"heb_Hebr\",\n",
    "    \"hi\": \"hin_Deva\",\n",
    "    \"hr\": \"hrv_Latn\",\n",
    "    \"hu\": \"hun_Latn\",\n",
    "    \"hy\": \"hye_Armn\",\n",
    "    \"nl\": \"nld_Latn\",\n",
    "    \"ig\": \"ibo_Latn\",\n",
    "    \"ilo\": \"ilo_Latn\",\n",
    "    \"id\": \"ind_Latn\",\n",
    "    \"is\": \"isl_Latn\",\n",
    "    \"it\": \"ita_Latn\",\n",
    "    \"jv\": \"jav_Latn\",\n",
    "    \"ja\": \"jpn_Jpan\",\n",
    "    \"kn\": \"kan_Knda\",\n",
    "    \"ka\": \"kat_Geor\",\n",
    "    \"kk\": \"kaz_Cyrl\",\n",
    "    \"km\": \"khm_Khmr\",\n",
    "    \"rw\": \"kin_Latn\",\n",
    "    \"ko\": \"kor_Hang\",\n",
    "    \"ku\": \"kmr_Latn\",\n",
    "    \"lo\": \"lao_Laoo\",\n",
    "    \"lv\": \"lvs_Latn\",\n",
    "    \"ln\": \"lin_Latn\",\n",
    "    \"lt\": \"lit_Latn\",\n",
    "    \"lb\": \"ltz_Latn\",\n",
    "    \"lg\": \"lug_Latn\",\n",
    "    \"lus\": \"lus_Latn\",\n",
    "    \"mai\": \"mai_Deva\",\n",
    "    \"ml\": \"mal_Mlym\",\n",
    "    \"mr\": \"mar_Deva\",\n",
    "    \"mk\": \"mkd_Cyrl\",\n",
    "    \"mg\": \"plt_Latn\",\n",
    "    \"mt\": \"mlt_Latn\",\n",
    "    \"mni-Mtei\": \"mni_Beng\",\n",
    "    \"mni\": \"mni_Beng\",\n",
    "    \"mn\": \"khk_Cyrl\",\n",
    "    \"mi\": \"mri_Latn\",\n",
    "    \"ms\": \"zsm_Latn\",\n",
    "    \"my\": \"mya_Mymr\",\n",
    "    \"no\": \"nno_Latn\",\n",
    "    \"ne\": \"npi_Deva\",\n",
    "    \"ny\": \"nya_Latn\",\n",
    "    \"om\": \"gaz_Latn\",\n",
    "    \"or\": \"ory_Orya\",\n",
    "    \"pl\": \"pol_Latn\",\n",
    "    \"pt\": \"por_Latn\",\n",
    "    \"ps\": \"pbt_Arab\",\n",
    "    \"qu\": \"quy_Latn\",\n",
    "    \"ro\": \"ron_Latn\",\n",
    "    \"ru\": \"rus_Cyrl\",\n",
    "    \"sa\": \"san_Deva\",\n",
    "    \"si\": \"sin_Sinh\",\n",
    "    \"sk\": \"slk_Latn\",\n",
    "    \"sl\": \"slv_Latn\",\n",
    "    \"sm\": \"smo_Latn\",\n",
    "    \"sn\": \"sna_Latn\",\n",
    "    \"sd\": \"snd_Arab\",\n",
    "    \"so\": \"som_Latn\",\n",
    "    \"es\": \"spa_Latn\",\n",
    "    \"sq\": \"als_Latn\",\n",
    "    \"sr\": \"srp_Cyrl\",\n",
    "    \"su\": \"sun_Latn\",\n",
    "    \"sv\": \"swe_Latn\",\n",
    "    \"sw\": \"swh_Latn\",\n",
    "    \"ta\": \"tam_Taml\",\n",
    "    \"te\": \"tel_Telu\",\n",
    "    \"tg\": \"tgk_Cyrl\",\n",
    "    \"tl\": \"tgl_Latn\",\n",
    "    \"th\": \"tha_Thai\",\n",
    "    \"ti\": \"tir_Ethi\",\n",
    "    \"ts\": \"tso_Latn\",\n",
    "    \"tk\": \"tuk_Latn\",\n",
    "    \"tr\": \"tur_Latn\",\n",
    "    \"ug\": \"uig_Arab\",\n",
    "    \"uk\": \"ukr_Cyrl\",\n",
    "    \"ur\": \"urd_Arab\",\n",
    "    \"uz\": \"uzn_Latn\",\n",
    "    \"vi\": \"vie_Latn\",\n",
    "    \"xh\": \"xho_Latn\",\n",
    "    \"yi\": \"ydd_Hebr\",\n",
    "    \"yo\": \"yor_Latn\",\n",
    "    \"zh-CN\": \"zho_Hans\",\n",
    "    \"zh\": \"zho_Hans\",\n",
    "    \"zh-TW\": \"zho_Hant\",\n",
    "    \"zu\": \"zul_Latn\",\n",
    "    \"pa\": \"pan_Guru\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda:0\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "# # NLLB API settings\n",
    "# HOST = \"winstxnhdw-nllb-api.hf.space\"\n",
    "# BASE = \"/api/v4/translator\"\n",
    "\n",
    "# def get_nllb_lang(lang):\n",
    "#     try:\n",
    "#         nllb_lang = nllb_langs[lang]\n",
    "#     except:\n",
    "#         nllb_lang = \"eng_Latn\"\n",
    "#     return nllb_lang\n",
    "\n",
    "# def api_translate_text(source: str, text: str) -> str:\n",
    "#     translated_text: str = \"\"\n",
    "#     if not text:\n",
    "#         warnings.warn(\"Cannot translate empty string!\")\n",
    "#         return translated_text\n",
    "\n",
    "#     limit = 2000  # chars\n",
    "#     source_lang = get_nllb_lang(source)\n",
    "\n",
    "#     for sentence in nltk.sent_tokenize(text):\n",
    "#         attempts = 1\n",
    "#         api_status = 429\n",
    "#         nllb_call = f'https://{HOST}{BASE}?text={sentence[:limit]}&source={source_lang}&target=eng_Latn'\n",
    "#         while attempts > 0 and api_status != 200:\n",
    "#             try:\n",
    "#                 nllb_response = requests.get(nllb_call, timeout=30)\n",
    "#                 api_status = nllb_response.status_code\n",
    "#                 if api_status == 200:\n",
    "#                     print(nllb_response.json())  # Print the entire response\n",
    "#                     translated_sentence = nllb_response.json().get(\"translation\", \"\")\n",
    "#                     translated_text = f\"{translated_text} {translated_sentence}\".strip()\n",
    "#                 else:\n",
    "#                     print(f\"Error: {nllb_response.status_code}, {nllb_response.text}\")\n",
    "#                     if api_status == 429:\n",
    "#                         time.sleep(15)\n",
    "#                         print('Slow down request and sleep for 15 seconds')\n",
    "#             except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):\n",
    "#                 print(\"Timeout\")\n",
    "#                 continue\n",
    "#             attempts -= 1\n",
    "\n",
    "#     return translated_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import nltk\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# NLLB API settings\n",
    "HOST = \"winstxnhdw-nllb-api.hf.space\"\n",
    "BASE = \"/api/v4/translator\"\n",
    "CUrl='https://winstxnhdw-nllb-api.hf.space/api/v4/translator?text=Hello&source=eng_Latn&target=spa_Latn'\n",
    "\n",
    "def get_nllb_lang(lang):\n",
    "    try:\n",
    "        nllb_lang = nllb_langs[lang]\n",
    "    except:\n",
    "        nllb_lang = \"eng_Latn\"\n",
    "    return nllb_lang\n",
    "\n",
    "# def api_translate_chunk(source: str, text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Translates a chunk of text without splitting it into sentences.\n",
    "#     \"\"\"\n",
    "#     translated_text = \"\"\n",
    "#     if not text:\n",
    "#         warnings.warn(\"Cannot translate empty string!\")\n",
    "#         return translated_text\n",
    "\n",
    "#     source_lang = get_nllb_lang(source)\n",
    "#     limit = 2000\n",
    "#     text = text[:limit]  # ensure within API char limit\n",
    "\n",
    "#     attempts = 3\n",
    "#     api_status = 429\n",
    "#     nllb_call = f'https://{HOST}{BASE}?text={text}&source={source_lang}&target=eng_Latn'\n",
    "\n",
    "#     while attempts > 0 and api_status != 200:\n",
    "#         try:\n",
    "#             nllb_response = requests.get(nllb_call, timeout=30)\n",
    "#             api_status = nllb_response.status_code\n",
    "#             if api_status == 200:\n",
    "#                 response_json = nllb_response.json()\n",
    "#                 # Extract only the result string\n",
    "#                 return response_json.get(\"result\", \"\")\n",
    "#             else:\n",
    "#                 print(f\"Error: {api_status}, {nllb_response.text}\")\n",
    "#                 if api_status == 429:\n",
    "#                     time.sleep(15)\n",
    "#                     print(\"429: Rate limit hit. Sleeping...\")\n",
    "#         except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):\n",
    "#             print(\"Timeout during API call.\")\n",
    "#         attempts -= 1\n",
    "\n",
    "#     return translated_text\n",
    "\n",
    "def api_translate_chunk(source: str, text: str, chunk_size: int = 1000) -> str:\n",
    "    \"\"\"\n",
    "    Translates long text by chunking it into fixed-length character chunks.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        warnings.warn(\"Cannot translate empty string!\")\n",
    "        return \"\"\n",
    "\n",
    "    source_lang = get_nllb_lang(source)\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    translated_parts = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        attempts = 3\n",
    "        api_status = 429\n",
    "        nllb_call = f'https://{HOST}{BASE}?text={chunk}&source={source_lang}&target=eng_Latn'\n",
    "\n",
    "        while attempts > 0 and api_status != 200:\n",
    "            try:\n",
    "                nllb_response = requests.get(nllb_call, timeout=30)\n",
    "                api_status = nllb_response.status_code\n",
    "\n",
    "                if api_status == 200:\n",
    "                    response_json = nllb_response.json()\n",
    "                    translated_chunk = response_json.get(\"result\", \"\")\n",
    "                    translated_parts.append(translated_chunk)\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"[Chunk {idx}] Error: {api_status}, {nllb_response.text}\")\n",
    "                    if api_status == 429:\n",
    "                        time.sleep(15)\n",
    "                        print(\"429: Rate limit hit. Sleeping...\")\n",
    "            except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):\n",
    "                print(f\"[Chunk {idx}] Timeout during API call.\")\n",
    "            attempts -= 1\n",
    "\n",
    "        if attempts == 0:\n",
    "            warnings.warn(f\"[Chunk {idx}] Failed after multiple attempts.\")\n",
    "            translated_parts.append(\"\")  # Keep chunk structure\n",
    "\n",
    "    # return \" \".join(translated_parts)\n",
    "\n",
    "    return \" \".join(translated_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_1  col_2\n",
       "0      1      2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({\"col_1\":1, \"col_2\":2}, index=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_large_text(source: str, text: str, chunk_size: int = 1000) -> str:\n",
    "#     if not text:\n",
    "#         warnings.warn(\"Cannot translate empty string!\")\n",
    "#         return \"\"\n",
    "\n",
    "#     # Chunk the text without breaking words or sentences if possible\n",
    "#     chunks = []\n",
    "#     current_chunk = \"\"\n",
    "\n",
    "#     for sentence in nltk.sent_tokenize(text):\n",
    "#         if len(current_chunk) + len(sentence) + 1 <= chunk_size:\n",
    "#             current_chunk += \" \" + sentence\n",
    "#         else:\n",
    "#             chunks.append(current_chunk.strip())\n",
    "#             current_chunk = sentence\n",
    "\n",
    "#     if current_chunk:\n",
    "#         chunks.append(current_chunk.strip())\n",
    "\n",
    "#     translated_chunks = []\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         #print(f\"Translating chunk {i+1}/{len(chunks)}...\")\n",
    "#         translated = api_translate_chunk(source, chunk)\n",
    "#         translated_chunks.append(translated)\n",
    "#         time.sleep(1)  # optional delay\n",
    "\n",
    "#     return \" \".join(translated_chunks).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=\"\"\"Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (1814–1889), in umliegende Dörfer ausliefern. Später arbeitete er als reisender Vertreter, dann als selbstständiger Grossist von Galanteriewaren in Prag. Julie Kafka gehörte einer wohlhabenden Familie aus Podiebrad an, verfügte über eine umfassendere Bildung als ihr Mann und hatte Mitspracherecht in dessen Geschäft, in dem sie täglich bis zu zwölf Stunden arbeitete.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_char_chunks(text, chunk_size=1000):\n",
    "#     return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# chunks = get_char_chunks(Text, chunk_size=1000)\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     print(f\"\\n--- Chunk {i + 1} ---\\n{chunk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"]=Text\n",
    "df[\"Translation\"] = df[\"text\"].apply(lambda x: api_translate_chunk(source=\"de\", text=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"Test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Le chef de l'ONU dit qu'il n'y a pas de solution militaire en Syrie\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AniKhvadagiani\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "article = \"UN Chief says there is no military solution in Syria\"\n",
    "inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"fra_Latn\"), max_length=30\n",
    ")\n",
    "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Der UN-Chef sagt, es gibt keine militärische Lösung in Syrien.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\", src_lang=\"ron_Latn\"\n",
    ")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n",
    "inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n",
    ")\n",
    "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Union, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ParagraphNLLBTranslator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"facebook/nllb-200-distilled-600M\",\n",
    "        device: str = None,\n",
    "        max_chunk_chars: int = 1000,\n",
    "        batch_size: int = 8,\n",
    "        torch_dtype: torch.dtype = torch.float16,\n",
    "        progress_bar: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Efficient paragraph translator using NLLB.\n",
    "        \n",
    "        Args:\n",
    "            model_name: NLLB model name/path\n",
    "            device: Force device (auto-detected if None)\n",
    "            max_chunk_chars: Maximum character length per chunk\n",
    "            batch_size: Translation batch size\n",
    "            torch_dtype: Model precision\n",
    "            progress_bar: Show progress bar during translation\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_chunk_chars = max_chunk_chars\n",
    "        self.batch_size = batch_size\n",
    "        self.progress_bar = progress_bar\n",
    "        \n",
    "        self.logger.info(f\"Loading {model_name} on {self.device}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch_dtype\n",
    "        ).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def translate_paragraphs(\n",
    "        self,\n",
    "        paragraphs: Union[str, List[str]],\n",
    "        source_lang: str,\n",
    "        target_lang: str,\n",
    "        num_beams: int = 4,\n",
    "        **generate_kwargs\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Translate paragraphs while preserving original structure.\n",
    "        \n",
    "        Args:\n",
    "            paragraphs: Single paragraph or list of paragraphs\n",
    "            source_lang: Source language code (e.g. 'eng_Latn')\n",
    "            target_lang: Target language code (e.g. 'fra_Latn')\n",
    "            num_beams: Beam search width\n",
    "            **generate_kwargs: Additional generation parameters\n",
    "            \n",
    "        Returns:\n",
    "            Translated text(s) in same format as input\n",
    "        \"\"\"\n",
    "        # Handle single paragraph case\n",
    "        is_single = isinstance(paragraphs, str)\n",
    "        if is_single:\n",
    "            paragraphs = [paragraphs]\n",
    "        \n",
    "        # Prepare language codes\n",
    "        source_lang = f'__{source_lang}__' if not source_lang.startswith('__') else source_lang\n",
    "        target_lang = f'__{target_lang}__' if not target_lang.startswith('__') else target_lang\n",
    "        \n",
    "        # Process all paragraphs\n",
    "        translated_paragraphs = []\n",
    "        iterator = tqdm(paragraphs, desc=\"Translating\") if self.progress_bar else paragraphs\n",
    "        \n",
    "        for para in iterator:\n",
    "            if not para.strip():  # Skip empty paragraphs\n",
    "                translated_paragraphs.append(\"\")\n",
    "                continue\n",
    "                \n",
    "            # Split long paragraphs into chunks\n",
    "            chunks = self._split_paragraph(para, self.max_chunk_chars)\n",
    "            translated_chunks = []\n",
    "            \n",
    "            # Process chunks in batches\n",
    "            for i in range(0, len(chunks), self.batch_size):\n",
    "                batch = chunks[i:i + self.batch_size]\n",
    "                translated_batch = self._translate_batch(\n",
    "                    batch,\n",
    "                    source_lang,\n",
    "                    target_lang,\n",
    "                    num_beams=num_beams,\n",
    "                    **generate_kwargs\n",
    "                )\n",
    "                translated_chunks.extend(translated_batch)\n",
    "            \n",
    "            # Reconstruct paragraph\n",
    "            translated_paragraphs.append(\" \".join(translated_chunks))\n",
    "        \n",
    "        return translated_paragraphs[0] if is_single else translated_paragraphs\n",
    "    \n",
    "    def _split_paragraph(\n",
    "        self,\n",
    "        paragraph: str,\n",
    "        max_chars: int\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Split paragraph into chunks respecting sentence boundaries.\"\"\"\n",
    "        if len(paragraph) <= max_chars:\n",
    "            return [paragraph]\n",
    "        \n",
    "        # First try to split at sentence boundaries\n",
    "        sentences = self._split_sentences(paragraph)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if len(current_chunk) + len(sent) + 1 <= max_chars:\n",
    "                current_chunk = f\"{current_chunk} {sent}\".strip()\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = sent\n",
    "                \n",
    "                # Handle very long sentences\n",
    "                if len(current_chunk) > max_chars:\n",
    "                    for i in range(0, len(current_chunk), max_chars):\n",
    "                        chunks.append(current_chunk[i:i + max_chars])\n",
    "                    current_chunk = \"\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Basic sentence splitting that works for most European languages.\"\"\"\n",
    "        # Improved sentence splitting that handles common cases\n",
    "        sentences = []\n",
    "        current_sentence = \"\"\n",
    "        \n",
    "        for char in text:\n",
    "            current_sentence += char\n",
    "            if char in {'.', '!', '?'}:\n",
    "                next_char = text[text.index(char) + 1] if text.index(char) + 1 < len(text) else ''\n",
    "                if next_char in {'\"', \"'\", \"”\", \")\", \"]\", \" \"} or next_char.isupper():\n",
    "                    sentences.append(current_sentence.strip())\n",
    "                    current_sentence = \"\"\n",
    "        \n",
    "        if current_sentence.strip():\n",
    "            sentences.append(current_sentence.strip())\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _translate_batch(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        source_lang: str,\n",
    "        target_lang: str,\n",
    "        num_beams: int = 4,\n",
    "        **generate_kwargs\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Efficient batch translation with error handling.\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=self.tokenizer.lang_code_to_id[target_lang],\n",
    "                num_beams=num_beams,\n",
    "                **generate_kwargs\n",
    "            )\n",
    "            \n",
    "            return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Translation error: {e}\")\n",
    "            # Return empty strings for failed translations to maintain output structure\n",
    "            return [\"\"] * len(texts)\n",
    "    \n",
    "    def get_supported_languages(self) -> List[str]:\n",
    "        \"\"\"Get list of supported language codes.\"\"\"\n",
    "        return [code.replace('__', '') for code in self.tokenizer.lang_code_to_id.keys() \n",
    "                if code.startswith('__') and code != \"__unk__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating:   0%|          | 0/1 [00:00<?, ?it/s]Translation error: NllbTokenizerFast has no attribute lang_code_to_id\n",
      "Translating: 100%|██████████| 1/1 [00:00<00:00, 31.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (1814–1889), in umliegende Dörfer ausliefern. Später arbeitete er als reisender Vertreter, dann als selbstständiger Grossist von Galanteriewaren in Prag. Julie Kafka gehörte einer wohlhabenden Familie aus Podiebrad an, verfügte über eine umfassendere Bildung als ihr Mann und hatte Mitspracherecht in dessen Geschäft, in dem sie täglich bis zu zwölf Stunden arbeitete.\n",
      "Neben den Brüdern Georg und Heinrich, die bereits als Kleinkinder starben, hatte Franz Kafka drei Schwestern, die später deportiert wurden, vermutlich in Konzentrationslager oder Ghettos, wo sich ihre Spuren verloren: Gabriele, genannt Elli (1889–1942?), Valerie, genannt Valli (1890–1942?), und Ottilie „Ottla“ Kafka (1892–1943). Da die Eltern tagsüber abwesend waren, wurden alle Geschwister im Wesentlichen von öfters wechselndem weiblichem Dienstpersonal aufgezogen.\n",
      "\n",
      "Translated:  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize translator\n",
    "translator = ParagraphNLLBTranslator(\n",
    "    model_name=\"facebook/nllb-200-distilled-600M\",\n",
    "    max_chunk_chars=1000,\n",
    "    batch_size=8,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Your paragraphs (could be thousands in a list)\n",
    "paragraphs = [\n",
    "\"\"\"Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (1814–1889), in umliegende Dörfer ausliefern. Später arbeitete er als reisender Vertreter, dann als selbstständiger Grossist von Galanteriewaren in Prag. Julie Kafka gehörte einer wohlhabenden Familie aus Podiebrad an, verfügte über eine umfassendere Bildung als ihr Mann und hatte Mitspracherecht in dessen Geschäft, in dem sie täglich bis zu zwölf Stunden arbeitete.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Translate to French\n",
    "translated = translator.translate_paragraphs(\n",
    "    paragraphs,\n",
    "    source_lang=\"eng_Latn\",\n",
    "    target_lang=\"deu_Latn\",\n",
    "    num_beams=4\n",
    ")\n",
    "\n",
    "# Output maintains original structure\n",
    "for orig, trans in zip(paragraphs, translated):\n",
    "    print(f\"Original: {orig}\")\n",
    "    print(f\"Translated: {trans}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "def load_nllb_model():\n",
    "    model_name = \"facebook/nllb-200-distilled-600M\"  # Medium-sized model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Language code mapping (NLLB uses slightly different codes)\n",
    "NLLB_LANG_CODES = {\n",
    "    'ar': 'ara_Arab', 'bg': 'bul_Cyrl', 'de': 'deu_Latn', 'el': 'ell_Grek',\n",
    "    'en': 'eng_Latn', 'es': 'spa_Latn', 'fr': 'fra_Latn', 'hi': 'hin_Deva',\n",
    "    'it': 'ita_Latn', 'ja': 'jpn_Jpan', 'nl': 'nld_Latn', 'pl': 'pol_Latn',\n",
    "    'pt': 'por_Latn', 'ru': 'rus_Cyrl', 'sw': 'swh_Latn', 'th': 'tha_Thai',\n",
    "    'tr': 'tur_Latn', 'ur': 'urd_Arab', 'vi': 'vie_Latn', 'zh': 'zho_Hans',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Function to split text into chunks\n",
    "def chunk_text(text, words_per_chunk=1000):\n",
    "    words = re.split(r'\\s+', text.strip())\n",
    "    chunks = [' '.join(words[i:i+words_per_chunk]) for i in range(0, len(words), words_per_chunk)]\n",
    "    return chunks\n",
    "\n",
    "# Function to translate a single chunk\n",
    "def translate_chunk(text, src_lang, model, tokenizer):\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    src_lang_code = NLLB_LANG_CODES.get(src_lang.lower(), 'eng_Latn')\n",
    "    \n",
    "    tokenizer.src_lang = src_lang_code\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    \n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(f\"<<eng_Latn>>\"),\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "# Main translation function\n",
    "def translate_text(row, model, tokenizer):\n",
    "    text = row['normalized_text']\n",
    "    src_lang = row['detected_language']\n",
    "    \n",
    "    if pd.isna(text) or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Chunk the text\n",
    "    chunks = chunk_text(text)\n",
    "    translated_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            translated = translate_chunk(chunk, src_lang, model, tokenizer)\n",
    "            translated_chunks.append(translated)\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating chunk: {e}\")\n",
    "            translated_chunks.append(\"[TRANSLATION ERROR]\")\n",
    "    \n",
    "    return ' '.join(translated_chunks)\n",
    "\n",
    "# Process your dataset\n",
    "def process_dataset(df):\n",
    "    print(\"Loading NLLB model...\")\n",
    "    model, tokenizer = load_nllb_model()\n",
    "    \n",
    "    print(\"Starting translation...\")\n",
    "    tqdm.pandas(desc=\"Translating texts\")\n",
    "    df['translated'] = df.progress_apply(lambda x: translate_text(x, model, tokenizer), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>detected_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Franz Kafkas Eltern, Hermann Kafka (1852–1931)...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     normalized_text detected_language\n",
       "0  Franz Kafkas Eltern, Hermann Kafka (1852–1931)...                de"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text=\"\"\"Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (1814–1889), in umliegende Dörfer ausliefern. Später arbeitete er als reisender Vertreter, dann als selbstständiger Grossist von Galanteriewaren in Prag. Julie Kafka gehörte einer wohlhabenden Familie aus Podiebrad an, verfügte über eine umfassendere Bildung als ihr Mann und hatte Mitspracherecht in dessen Geschäft, in dem sie täglich bis zu zwölf Stunden arbeitete.\n",
    "\"\"\"\n",
    "# Load your dataset\n",
    "df = pd.DataFrame({\"normalized_text\":Text,\"detected_language\":\"de\"}, index=[0])\n",
    "df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLLB model...\n",
      "Starting translation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating texts: 100%|██████████| 1/1 [00:29<00:00, 29.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "translated_df = process_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 219\n"
     ]
    }
   ],
   "source": [
    "text = Text\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")  # No truncation\n",
    "print(f\"Token count: {len(tokens['input_ids'][0])}\")  # If >1024, truncation occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franz Kafkas Eltern, Hermann Kafka (18521931) and Julie Löwy (18561934), established the bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polish kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, where in simple conditions aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (18141889), in umliegende Dörfer ausliefassfern. Späteritete er als reisender Vertreter, dann als selbst Grossist von Galanteriewaren in Prag. Julie Kafka may have a well-habited Family austere, mit einer Bildung über eine um um arbeitete und ihre Arbechte in der Geschäftsbeziehung, also known as a bishop in Munich.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def translate_german_to_english(text, model_name=\"facebook/nllb-200-distilled-600M\", max_length=1024):\n",
    "    \"\"\"\n",
    "    Translates German text to English using NLLB model with chunking.\n",
    "    \n",
    "    Args:\n",
    "        text: German text to translate\n",
    "        model_name: NLLB model identifier (default is 600M parameter distilled version)\n",
    "        max_length: Maximum token length for each chunk (default 1024)\n",
    "    \n",
    "    Returns:\n",
    "        Translated English text\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the entire text to determine chunks\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    \n",
    "    # Split into chunks of max_length tokens\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_ids), max_length):\n",
    "        chunk = input_ids[i:i + max_length]\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    # Translate each chunk\n",
    "    translated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        # Create attention mask for the chunk\n",
    "        attention_mask = (chunk != tokenizer.pad_token_id).int()\n",
    "        \n",
    "        # Generate translation\n",
    "        outputs = model.generate(\n",
    "            input_ids=chunk.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0),\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"),\n",
    "            max_length=max_length + 100  # Allow some extra tokens for translation\n",
    "        )\n",
    "        \n",
    "        # Decode the translation\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        translated_chunks.append(translated_text)\n",
    "    \n",
    "    # Combine all translated chunks\n",
    "    full_translation = \" \".join(translated_chunks)\n",
    "    \n",
    "    # Clean up any artifacts from chunking\n",
    "    full_translation = full_translation.replace(\" .\", \".\").replace(\" ,\", \",\")\n",
    "    full_translation = \" \".join(full_translation.split())  # Normalize whitespace\n",
    "    \n",
    "    return full_translation\n",
    "\n",
    "# Example usage\n",
    "german_text = \"\"\"\n",
    "Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs. Er musste als Kind die Waren seines Vaters, des Schächters Jakob Kafka (1814–1889), in umliegende Dörfer ausliefern. Später arbeitete er als reisender Vertreter, dann als selbstständiger Grossist von Galanteriewaren in Prag. Julie Kafka gehörte einer wohlhabenden Familie aus Podiebrad an, verfügte über eine umfassendere Bildung als ihr Mann und hatte Mitspracherecht in dessen Geschäft, in dem sie täglich bis zu zwölf Stunden arbeitete.\n",
    "\"\"\"\n",
    "\n",
    "english_translation = translate_german_to_english(german_text)\n",
    "print(english_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AniKhvadagiani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Franz Kafka's parents, Hermann Kafka (18521931) and Julie Löwy (18561934), were from bourgeois Jewish merchant families.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "# Make sure to download punkt for sentence tokenization\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Define language codes\n",
    "src_lang = \"deu_Latn\"\n",
    "tgt_lang = \"eng_Latn\"\n",
    "tokenizer.src_lang = src_lang\n",
    "\n",
    "# Split German text into sentences (cleaner than hard-chunking)\n",
    "def split_sentences(text, max_token_len=1024):\n",
    "    sentences = sent_tokenize(text, language=\"german\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Try adding sentence to the current chunk\n",
    "        test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
    "        tokens = tokenizer(test_chunk, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "\n",
    "        if len(tokens) <= max_token_len:\n",
    "            current_chunk = test_chunk\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Translate with forced BOS token\n",
    "def translate_text(text):\n",
    "    chunks = split_sentences(text)\n",
    "    bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "    translations = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        with torch.no_grad():\n",
    "            translated_tokens = model.generate(**inputs, forced_bos_token_id=bos_token_id)\n",
    "        translation = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        translations.append(translation)\n",
    "\n",
    "    return \" \".join(translations)\n",
    "\n",
    "# Example\n",
    "german_text = \"\"\"Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), stammten aus bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich von der Dohle ab, tschechisch kavka, polnisch kawka.\"\"\"\n",
    "translated = translate_text(german_text)\n",
    "print(translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AniKhvadagiani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Translating chunk 1/1:\n",
      "Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), stammten aus bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich von der Dohle ab, tschechisch kavka, polnisch kawka.\n",
      "\n",
      "✅ Translation:\n",
      "Franz Kafka's parents, Hermann Kafka (18521931) and Julie Löwy (18561934), were from bourgeois Jewish merchant families.\n",
      "\n",
      "\n",
      "🔸 Full Translation Result:\n",
      " Franz Kafka's parents, Hermann Kafka (18521931) and Julie Löwy (18561934), were from bourgeois Jewish merchant families.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Set source language\n",
    "src_lang = \"deu_Latn\"\n",
    "tgt_lang = \"eng_Latn\"\n",
    "tokenizer.src_lang = src_lang\n",
    "bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "\n",
    "# Split into sentence-based chunks\n",
    "def split_sentences(text, max_token_len=1024):\n",
    "    sentences = sent_tokenize(text, language=\"german\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        temp_chunk = f\"{current_chunk} {sentence}\".strip() if current_chunk else sentence\n",
    "        tokenized = tokenizer(temp_chunk, return_tensors=\"pt\", truncation=False)\n",
    "        if tokenized[\"input_ids\"].shape[1] <= max_token_len:\n",
    "            current_chunk = temp_chunk\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Translate text chunk-by-chunk\n",
    "def translate_text(text):\n",
    "    chunks = split_sentences(text)\n",
    "    translations = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\n🔹 Translating chunk {i+1}/{len(chunks)}:\\n{chunk}\\n\")\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        with torch.no_grad():\n",
    "            output_tokens = model.generate(**inputs, forced_bos_token_id=bos_token_id)\n",
    "        translation = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "        translations.append(translation)\n",
    "        print(f\"✅ Translation:\\n{translation}\\n\")\n",
    "\n",
    "    return \" \".join(translations)\n",
    "\n",
    "# Example input\n",
    "german_text = \"\"\"\n",
    "Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), stammten aus bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich von der Dohle ab, tschechisch kavka, polnisch kawka.\n",
    "\"\"\"\n",
    "\n",
    "result = translate_text(german_text)\n",
    "print(\"\\n🔸 Full Translation Result:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The first is the German-speaking population, which is the largest in the European Union. The family name is derived from the name of the doll, Czech kavka, Polish kawka, ab. Der Vater comes from the village of Wosek in Südböhmen, where he grew up in simple circumstances.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import warnings\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "HOST = \"winstxnhdw-nllb-api.hf.space\"\n",
    "BASE = \"/api/v4/translator\"\n",
    "import requests\n",
    "\n",
    "def get_nllb_lang(lang):\n",
    "    try:\n",
    "        nllb_lang = nllb_langs[lang]\n",
    "    except:\n",
    "        nllb_lang = \"eng_Latn\"\n",
    "    return nllb_lang\n",
    "def api_translate_text(source: str, text: str) -> str:\n",
    "    translated_text: str = \"\"\n",
    "    if not text:\n",
    "        warnings.warn(\"Cannot translate empty string!\")\n",
    "        return translated_text\n",
    "\n",
    "    limit = 2000  # chars\n",
    "    source_lang = get_nllb_lang(source)\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        attempts = 10\n",
    "        api_status = 429\n",
    "        nllb_call = f'https://{HOST}{BASE}?text={sentence[:limit]}&source={source_lang}&target=eng_Latn'\n",
    "        while attempts > 0 and api_status != 200:\n",
    "            try:\n",
    "                nllb_response = requests.get(nllb_call, timeout=30)\n",
    "                \n",
    "            except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):\n",
    "                print(\"Timeout\")\n",
    "                continue\n",
    "            print(nllb_response.status_code)\n",
    "            api_status = nllb_response.status_code\n",
    "            attempts -= 1\n",
    "            if api_status == 200:\n",
    "                response_json = nllb_response.json()\n",
    "                if \"result\" in response_json:\n",
    "                    translated_sentence = response_json[\"result\"]\n",
    "                    translated_text += \" \" + translated_sentence if translated_text else translated_sentence\n",
    "                else:\n",
    "                    print(\"Unexpected response format:\", response_json)\n",
    "                    break\n",
    "\n",
    "    return translated_text\n",
    "api_translate_text(source = 'de', text='Franz Kafkas Eltern, Hermann Kafka (1852–1931) und Julie Löwy (1856–1934), entstammten bürgerlichen jüdischen Kaufmannsfamilien. Der Familienname leitet sich vom Namen der Dohle, tschechisch kavka, polnisch kawka, ab. Der Vater kam aus dem Dorf Wosek in Südböhmen, wo er in einfachen Verhältnissen aufwuchs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: 24 hours Delivery: 49€ (2987 Trustpilot reviews) New products OUTLET Our brands\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def translate_large_text_to_english(\n",
    "    text: str, \n",
    "    source_lang: str = \"auto\", \n",
    "    max_chunk_size: int = 1000,\n",
    "    model_name: str = \"facebook/nllb-200-distilled-600M\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translates large text to English using the NLLB model.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to translate\n",
    "        source_lang: Source language code (e.g., 'fra_Latn' for French) or 'auto' for auto-detection\n",
    "        max_chunk_size: Maximum character length for each chunk (to avoid token limits)\n",
    "        model_name: Name of the NLLB model to use\n",
    "        \n",
    "    Returns:\n",
    "        Translated text in English\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # If language is auto, detect it (simple heuristic for example)\n",
    "    if source_lang == \"auto\":\n",
    "        # This is a simple heuristic - for better detection you might want a separate language ID model\n",
    "        if re.search(r\"[а-яА-Я]\", text):\n",
    "            source_lang = \"rus_Cyrl\"  # Russian\n",
    "        elif re.search(r\"[äöüßÄÖÜ]\", text):\n",
    "            source_lang = \"deu_Latn\"  # German\n",
    "        else:\n",
    "            source_lang = \"fra_Latn\"  # Default to French if unknown\n",
    "    \n",
    "    # Target language is always English\n",
    "    tgt_lang = \"eng_Latn\"\n",
    "    \n",
    "    # Split text into manageable chunks\n",
    "    chunks = _split_text_into_chunks(text, max_chunk_size)\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            # Prepare the text with language codes\n",
    "            inputs = tokenizer(\n",
    "                f\"{source_lang} {chunk}\",\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=tokenizer.model_max_length\n",
    "            )\n",
    "            \n",
    "            # Generate translation\n",
    "            translated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            # Decode and clean up\n",
    "            translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "            translated_chunks.append(translated_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating chunk: {str(e)}\")\n",
    "            translated_chunks.append(chunk)  # Fallback to original text if error\n",
    "    \n",
    "    return \" \".join(translated_chunks)\n",
    "\n",
    "def _split_text_into_chunks(text: str, max_chunk_size: int) -> List[str]:\n",
    "    \"\"\"Split text into chunks trying to preserve paragraphs where possible.\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    # Split by paragraphs first\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        if len(current_chunk) + len(para) < max_chunk_size:\n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            # If single paragraph is too long, split by sentences\n",
    "            if len(para) > max_chunk_size:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "                current_sentence_chunk = \"\"\n",
    "                for sentence in sentences:\n",
    "                    if len(current_sentence_chunk) + len(sentence) < max_chunk_size:\n",
    "                        current_sentence_chunk += sentence + \" \"\n",
    "                    else:\n",
    "                        if current_sentence_chunk:\n",
    "                            chunks.append(current_sentence_chunk.strip())\n",
    "                            current_sentence_chunk = sentence + \" \"\n",
    "                if current_sentence_chunk:\n",
    "                    chunks.append(current_sentence_chunk.strip())\n",
    "            else:\n",
    "                chunks.append(para)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "text = \"PRECIOVS - montre et bijou tendances au meilleur prix - Livraison 24h Livraison offerte dès 49€ |       (2987 avis Trustpilot) Nouveautés OUTLET Nos marques Montres Bijoux Maroquinerie Lunettes de soleil Accessoires Chèques et coffrets cadeaux Besoin d'aide ? +32 499 27 84 11 i\"\n",
    "translated_text = translate_large_text_to_english(text, source_lang=\"fra_Latn\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"148.251.182.183\"\n",
    "PORT = \"6060\"\n",
    "BASE = \"/RTOKaprU81iaxv_vjay+i324\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nllb_lang(lang):\n",
    "    try:\n",
    "        nllb_lang = nllb_langs[lang]\n",
    "    except:\n",
    "        nllb_lang = \"eng_Latn\"\n",
    "    return nllb_lang\n",
    "\n",
    "\n",
    "def api_translate_text(source: str, text: str) -> str:\n",
    "    translated_text: str = \"\"\n",
    "    if not text:\n",
    "        warnings.warn(\"Cannot translate empty string!\")\n",
    "        return translated_text\n",
    "\n",
    "    limit = 2000  # chars\n",
    "    source_lang = get_nllb_lang(source)\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        attempts = 10\n",
    "        api_status = 429\n",
    "        nllb_call = 'http://' + HOST + ':' + PORT + BASE + '/translate?source=' + sentence[:limit] + '&src_lang=' + source_lang + '&tgt_lang=eng_Latn'\n",
    "        while attempts > 0 and api_status != 200:\n",
    "            try:\n",
    "                nllb_response = requests.get(nllb_call, timeout=30)\n",
    "            except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout):\n",
    "                print(\"Timeout\")\n",
    "                continue\n",
    "            #print(nllb_response.status_code)\n",
    "            api_status = nllb_response.status_code\n",
    "            attempts -= 1\n",
    "            if api_status == 200:\n",
    "                translated_sentence = nllb_response.json()[\"translation\"][0]\n",
    "                if len(translated_text) > 0:\n",
    "                    translated_text = translated_text + \" \" + translated_sentence\n",
    "                else:\n",
    "                    translated_text = translated_sentence\n",
    "            elif api_status != 200:\n",
    "                time.sleep(15)\n",
    "                print('Slow down request and sleep for 15 seconds')\n",
    "            else:\n",
    "                print('Another error occurred')\n",
    "\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Johann Wolfgang Goethe, born in 1782. The following is a list of the Member States of the European Union whose currency is the euro: March 1832 in Weimar) was a German poet, politician and naturalist. He is considered one of the most important creators of German-language poetry. Goethe came from a wealthy and respected bourgeois family; his maternal grandfather was the highest judicial official in the city of Frankfurt am Main as a city councillor, his father a doctor of law and Imperial Council. He and his sister Cornelia received extensive training from home teachers. Following his father\\'s wishes, Goethe studied law in Leipzig and Strasbourg and then worked as a lawyer in Wetzlar and Frankfurt. At the same time, he pursued his penchant for poetry. He achieved his first recognition in the world of literature in 1773 with the drama God of Berlichingen, which brought him national success, and in 1774 with the epistolary novel The Suffering of the Young Werther, with which he even achieved European success. Both works are to be ascribed to the literary movement of the Storm and the Urge (1765 to 1785). The young Duke Carl August of Saxony-Weimar-Eisenach summoned the 26-year-old Goethe to his court in Weimar in 1775, where he eventually remained for the rest of his life. He held political and administrative offices there as a friend and minister of the Duke and directed the Weimar Court Theatre for a quarter of a century. The neglect of his creative abilities resulting from his official activity triggered a personal crisis after the first Weimar decade, which Goethe escaped by fleeing to Italy. The trip to Italy from September 1786 to May 1788 was a \"rebirth\" for him. He was credited with completing important works such as Iphigenia on Tauris (1787), Egmont (1788) and Torquato Tasso (1790). After his return, his duties were largely limited to representative tasks. The richness of his cultural heritage in Italy stimulated his poetic output, and the erotic experiences with a young Roman woman led him to enter into a lasting, \"uncommon\" love affair with Christiane Vulpius immediately upon his return, which he did not legalize until eighteen years later with a marriage. Goethe\\'s literary work includes poetry, drama, epic, autobiographical, artistic and literary theoretical, and natural science writings. In addition, his extensive correspondence is of literary significance. Even Napoleon asked him to an audience at the Congress of the Earls. In association with Schiller and together with Herder and Wieland, he embodied the Weimar Classical. The Wilhelm-Meister novels became exemplary precursors of German-language artistic and educational novels. His play Faust (1808) gained a reputation as the most important work of German-language literature. In his old age he was also regarded abroad as a representative of the spiritual Germany.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#api_translate_text(source=\"de\", text=Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from urllib.parse import quote_plus\n",
    "# import time  # optional, for adding delay between requests if needed\n",
    "\n",
    "# # Server configuration\n",
    "# HOST = \"148.251.182.183\"\n",
    "# PORT = \"6060\"\n",
    "# BASE = \"/RTOKaprU81iaxv_vjay%2Bi324\"  # + must be encoded\n",
    "\n",
    "# # Translation function\n",
    "# def translate_large_text(text, source_lang=\"fra_Latn\", chunk_size=500):\n",
    "#     \"\"\"\n",
    "#     Translates large input text by chunking and sending each part via POST request.\n",
    "    \n",
    "#     :param text: The full input text to translate\n",
    "#     :param source_lang: Language code of the input text (e.g. 'fra_Latn')\n",
    "#     :param chunk_size: Approximate number of words per request\n",
    "#     :return: Translated full text\n",
    "#     \"\"\"\n",
    "#     # Split text into chunks\n",
    "#     words = text.split()\n",
    "#     chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "#     translated_chunks = []\n",
    "\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         url = f\"http://{HOST}:{PORT}{BASE}/translate\"\n",
    "#         payload = {\n",
    "#             \"source\": chunk,\n",
    "#             \"src_lang\": source_lang,\n",
    "#             \"tgt_lang\": \"eng_Latn\"\n",
    "#         }\n",
    "\n",
    "#         try:\n",
    "#             response = requests.post(url, json=payload)\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 translated = data.get(\"translation\", \"\")\n",
    "#                 if isinstance(translated, list):\n",
    "#                     translated = ' '.join(translated)\n",
    "#                 elif not isinstance(translated, str):\n",
    "#                     translated = str(translated)\n",
    "#                 translated_chunks.append(translated)\n",
    "#                 print(f\"Translated chunk {i + 1}/{len(chunks)}\")\n",
    "#             else:\n",
    "#                 print(f\"Error translating chunk {i + 1}: Status {response.status_code}\")\n",
    "#                 translated_chunks.append(\"[Translation failed]\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Exception during translation of chunk {i + 1}: {e}\")\n",
    "#             translated_chunks.append(\"[Exception occurred]\")\n",
    "\n",
    "#         time.sleep(0.2)  # Optional delay between requests\n",
    "\n",
    "#     return '\\n'.join(translated_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=\"\"\"Johann Wolfgang Goethe, ab 1782 von Goethe (* 28. August 1749 in Frankfurt am Main; † 22. März 1832 in Weimar), war ein deutscher Dichter, Politiker und Naturforscher. Er gilt als einer der bedeutendsten Schöpfer deutschsprachiger Dichtung.\n",
    "\n",
    "Goethe stammte aus einer wohlhabenden und angesehenen bürgerlichen Familie; sein Großvater mütterlicherseits war als Stadtschultheiß höchster Justizbeamter der Stadt Frankfurt am Main, sein Vater Doktor der Rechte und Kaiserlicher Rat. Er und seine Schwester Cornelia erfuhren eine aufwendige Ausbildung durch Hauslehrer. Dem Wunsch seines Vaters folgend, studierte Goethe in Leipzig und Straßburg Rechtswissenschaft und war danach als Advokat in Wetzlar und Frankfurt tätig. Gleichzeitig folgte er seiner Neigung zur Dichtkunst. Die ersten Anerkennungen in der Welt der Literatur erzielte er 1773 mit dem Drama Götz von Berlichingen, das ihm nationalen Erfolg eintrug, und 1774 mit dem Briefroman Die Leiden des jungen Werthers, mit dem er sogar europäischen Erfolg erreichte. Beide Werke sind der literarischen Strömung des Sturm und Drang (1765 bis 1785) zuzuordnen.\n",
    "\n",
    "Der jugendliche Herzog Carl August von Sachsen-Weimar-Eisenach berief den 26-jährigen Goethe 1775 an seinen Hof in Weimar, wo er schließlich für den Rest seines Lebens blieb. Er bekleidete dort als Freund und Minister des Herzogs politische und administrative Ämter und leitete ein Vierteljahrhundert das Weimarer Hoftheater. Die aus seiner amtlichen Tätigkeit resultierende Vernachlässigung seiner schöpferischen Fähigkeiten löste nach dem ersten Weimarer Jahrzehnt eine persönliche Krise aus, der sich Goethe durch die Flucht nach Italien entzog. Die Italienreise von September 1786 bis Mai 1788 empfand er als eine „Wiedergeburt“. Ihr verdankte er die Vollendung wichtiger Werke wie Iphigenie auf Tauris (1787), Egmont (1788) und Torquato Tasso (1790).\n",
    "\n",
    "Nach seiner Rückkehr wurden seine Amtspflichten weitgehend auf repräsentative Aufgaben beschränkt. Der in Italien erlebte Reichtum an kulturellem Erbe stimulierte seine dichterische Produktion, und die erotischen Erlebnisse mit einer jungen Römerin ließen ihn unmittelbar nach seiner Rückkehr eine dauerhafte, „unstandesgemäße“ Liebesbeziehung zu Christiane Vulpius aufnehmen, die er erst achtzehn Jahre später mit einer Eheschließung legalisierte.\n",
    "\n",
    "Goethes literarisches Werk umfasst Lyrik, Dramen, Epik, autobiografische, kunst- und literaturtheoretische sowie naturwissenschaftliche Schriften. Daneben ist sein umfangreicher Briefwechsel von literarischer Bedeutung. Selbst Napoleon bat ihn zu einer Audienz anlässlich des Erfurter Fürstenkongresses. Im Bunde mit Schiller und gemeinsam mit Herder und Wieland verkörperte er die Weimarer Klassik. Die Wilhelm-Meister-Romane wurden zu beispielgebenden Vorläufern deutschsprachiger Künstler- und Bildungsromane. Sein Drama Faust (1808) errang den Ruf als die bedeutendste Schöpfung der deutschsprachigen Literatur. Im Alter wurde er auch im Ausland als Repräsentant des geistigen Deutschlands angesehen.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated chunk 1/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Johann Wolfgang Goethe, ab 1782 von Goethe (* 28 August 1749 in Frankfurt am Main; † 22 March 1832 in Weimar), was a German poet, politician and naturalist.He is considered one of the most important creators of German-language poetry.Goethe came from a wealthy and respected bourgeois family; his grandfather on his mother's side was as Stadtschultheiß höchster Justizbeamter of the city of Frankfurt am Main, his father Doctor of Laws and Kaiseräslicher Rat.He and his sister Cornelia received a formal education through a teacher.Goethe studied at the Hof der Rechtsgelegenheiten in Leipzig and Straßburg, where his father's works The German Literary and Drama were published.At the same time, his literary and literary works The German Literary and Drama were of great importance to the German literary world.At the same time, his literary works The German Literary and Drama were of great cultural importance to the German literary world.Weimar literature was also an important source of cultural heritage.Weimar literature was the first source of literature for the German literary world.He was also known for his work The German Literary and Political Literature.He was a leading authority on the German literary world.He was known for his work The German Literature of the First World (1780), The German Literature of the First World (1780), The German Literature of the First World (1780), The German Literature of the First World (1780 and the German Literature (1780), The German Literature of the First World (1780 and the German Literature (1780 and the German Literature), The German Literature of the First World (1780 and the German Literature (1780 and the German Literature), The German Literature of the First World (1780\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_large_text(source_lang=\"de\", text=Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
