{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing: Clean and preprocess the extracted HTML data:\n",
    "\n",
    "Remove HTML tags and scripts.\n",
    "Normalize text (lowercasing, removing punctuation).\n",
    "Tokenization and stop-word removal can be beneficial.\n",
    "\n",
    "\n",
    "Feature Extraction: Identify and extract relevant features that could help classify a domain as an eCommerce store:\n",
    "\n",
    "Keywords and phrases.\n",
    "HTML structure elements (e.g., presence of <product>, <price>, <cart> tags).\n",
    "Meta tags relevant to eCommerce.\n",
    "\n",
    "\n",
    "LLM Utilization:\n",
    "\n",
    "Use large language models (like GPT-3, BERT) to classify extracted text or assist in feature selection. Fine-tuning these models on your labeled data could yield excellent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import warnings\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import spacy\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=pd.read_csv('noshop.csv').iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phrases to search for\n",
    "# phrases_to_search = [\n",
    "#     \"Terms and conditions\",\n",
    "#     \"Privacy policy\",\n",
    "#     \"Return and exchange policy\",\n",
    "#     \"Place a return or exchange request\",\n",
    "#     \"Track your order\",\n",
    "#     \"Add to cart\",\n",
    "#     \"add to cart\",\n",
    "#     \"select product\",\n",
    "#     \"Online shop\",\n",
    "#     \"Shipping & Payment\",\n",
    "#     \"Exchanges & Refunds\",\n",
    "#     \"How to Shop\",\n",
    "#     \"Order Status\",\n",
    "#     \"Shipping Method\",\n",
    "#     \"Trackin order\",\n",
    "#     \"User Guide \",\n",
    "#     \"online e-commerce\",\n",
    "#     \"Quick buy\",\n",
    "#     \"Easy Return Guidance\",\n",
    "#     \"Shipping Details\",\n",
    "#     \"Online shop\",\n",
    "#     \"Delivery\",\n",
    "#     \"RETURNS AND EXCHANGE\",  \n",
    "#     \"Payment methods\",\n",
    "#     \"Returns and refunds\",\n",
    "#     \"Shipping Information & Costs\",\n",
    "#     \"Shopping basket\",\n",
    "#     \"Loyalte program\",\n",
    "#     \"Gift Cards\",\n",
    "#     \"Shipping and payment\",\n",
    "#     \"Returns / Product exchange\",\n",
    "#     \"Payment\", \n",
    "#     \"Shipping & Delivery\",\n",
    "#     \"Exchanges & Returns\",\n",
    "#     \"Shipping and Exchange Policy\",\n",
    "#     \"Shopping basket\",\n",
    "#     \"Refund policy\",\n",
    "#     \"All about shopping\",\n",
    "#     \"Basket\",\n",
    "#     \"Shopping cart\",\n",
    "#     \"Return procedure\",\n",
    "#     \"Payment methods\",\n",
    "#     \"Guarantees and returns\",\n",
    "#     \"Return/Complaint Instructions\",\n",
    "#     \"About Click & Collect\", # \"https://www.bike-online.jp/hpgen/HPB/entries/43.html\"\n",
    "#     \"E-shop\",\n",
    "#     \"order status \"\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppress SSL warnings\n",
    "# # warnings.filterwarnings(\"ignore\", category=requests.packages.urllib3.exceptions.InsecureRequestWarning)\n",
    "# warnings.simplefilter(action='ignore')\n",
    "# # Function to fetch HTML content from the domain\n",
    "# def fetch_html(domain):\n",
    "#     headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "#     }\n",
    "#     try:\n",
    "#         response = requests.get(f'https://{domain}', headers=headers, timeout=10, verify=False)\n",
    "#         if response.status_code == 200:\n",
    "#             return response.text\n",
    "#         else:\n",
    "#             return \"\"\n",
    "#     except requests.RequestException:\n",
    "#         return \"\"\n",
    "\n",
    "# # Function to extract and clean text from the HTML\n",
    "# def extract_text_from_domain(domain):\n",
    "#     # Fetch HTML content using fetch_html\n",
    "#     html = fetch_html(domain)\n",
    "#     if not html:\n",
    "#         return \"Error fetching the webpage.\"\n",
    "\n",
    "#     try:\n",
    "#         soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#         # Extract text from all elements and clean it\n",
    "#         text = soup.get_text(separator=\" \")\n",
    "#         cleaned_text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "#         return cleaned_text\n",
    "#     except Exception as e:\n",
    "#         return f\"Error extracting text: {e}\"\n",
    "\n",
    "# # Function to search for specific phrases\n",
    "# def search_phrases(html_content, phrases):\n",
    "#     if not isinstance(html_content, str):\n",
    "#         return ''\n",
    "#     found_phrases = []\n",
    "#     for phrase in phrases:\n",
    "#         if phrase.lower() in html_content.lower():\n",
    "#             found_phrases.append(phrase)\n",
    "#     return ', '.join(found_phrases)\n",
    "\n",
    "# # Function to translate text\n",
    "# def translate_text(text, src_language):\n",
    "#     translator = Translator()\n",
    "#     translation = translator.translate(text, src=src_language, dest=\"en\")\n",
    "\n",
    "# # Function to detect the HTML language\n",
    "# def detect_language(html_content):\n",
    "#     if not html_content:\n",
    "#         return None\n",
    "    \n",
    "#     # Use regex to find the lang attribute directly\n",
    "#     lang_match = re.search(r'<html[^>]*\\blang=[\"\\']?([a-zA-Z-]+)', html_content, re.IGNORECASE)\n",
    "#     if lang_match:\n",
    "#         detected_lang = lang_match.group(1).split('-')[0]  # Extract primary language\n",
    "#         return detected_lang\n",
    "\n",
    "#     # Fallback to BeautifulSoup parsing\n",
    "#     soup = BeautifulSoup(html_content, 'lxml')\n",
    "#     html_tag = soup.html\n",
    "#     if html_tag and html_tag.get('lang'):\n",
    "#         detected_lang = html_tag.get('lang').split('-')[0]\n",
    "#         return detected_lang\n",
    "#     return \"\"\n",
    "\n",
    "# # Apply functions to DataFrame\n",
    "# data_1['HTML'] = data_1['domain'].apply(fetch_html)\n",
    "# data_1['Text'] = data_1['domain'].apply(extract_text_from_domain)\n",
    "# data_1['found_phrases'] = data_1['HTML'].apply(lambda html: search_phrases(html, phrases_to_search))\n",
    "# data_1['language'] = data_1['HTML'].apply(detect_language)\n",
    "# data_1.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1[data_1['HTML'].apply(lambda x: x not in [[], \"\"])]\n",
    "data_1 = data_1[data_1[\"Text\"].apply(lambda x: not isinstance(x, float))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_symbols_to_change = {\n",
    "    \"\\u2714\": ' ',  # ✔\n",
    "    \"\\u2605\": ' ',  # ★\n",
    "    \"\\u00A9\": ' ',  # ©\n",
    "    \"\\u00AE\": ' ',  # ®\n",
    "    \"\\u2022\": ' ',  # •\n",
    "    \"\\u2798\": ', ',  # ➨\n",
    "    \"\\u27BD\": ', ',  # ➽\n",
    "    \"\\u2661\": ', ',  # ♡\n",
    "    \"\\u266A\": ', ',  # ♪\n",
    "    \"\\u2026\": ', ',  # …\n",
    "    \"\\u25CF\": ', ',  # ●\n",
    "    \"\\u2122\": ' ',  # ™\n",
    "    \"\\u2713\": ' ',  # ✓\n",
    "    \"\\u2600\": ' ',  # ☀ (Stars and Sun)\n",
    "    \"\\u2665\": ' ',  # ♥ \n",
    "    \"\\U000025BA\": ' ', # ►\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_symbols(text):\n",
    "    \"\"\"\n",
    "    Replace some symbols in text as defined in lookup.dict_symbols_to_change.\n",
    "    \"\"\"\n",
    "    for key, value in dict_symbols_to_change.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')  # Replace emojis with empty string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Text\"]=data_1[\"Text\"].apply(replace_symbols)\n",
    "data_1[\"Text\"]=data_1[\"Text\"].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>HTML</th>\n",
       "      <th>Text</th>\n",
       "      <th>found_phrases</th>\n",
       "      <th>language</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>arena.pl</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"pl\"&gt;&lt;head&gt;&lt;link rel...</td>\n",
       "      <td>Arena.pl   Platforma zakupowa   Bezpieczne zak...</td>\n",
       "      <td>Delivery, Payment, Basket, E-shop</td>\n",
       "      <td>pl</td>\n",
       "      <td>1079239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>anzlitlovers.com</td>\n",
       "      <td>&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...</td>\n",
       "      <td>ANZ LitLovers LitBlog | For lovers of Australi...</td>\n",
       "      <td>Delivery, Payment</td>\n",
       "      <td>en</td>\n",
       "      <td>241540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>arikair.com</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html class=\"load-full-screen...</td>\n",
       "      <td>Fly Arik Air - West-Africa's leading airline o...</td>\n",
       "      <td>Terms and conditions, Delivery, Payment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>android8o.com</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\r\\n&lt;html lang=\"ru\"&gt;\\r\\n&lt;head&gt;\\...</td>\n",
       "      <td>Прошивки Android Oreo 8 для смартфонов и планш...</td>\n",
       "      <td>Basket</td>\n",
       "      <td>ru</td>\n",
       "      <td>98454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>apiajapan.com</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\t&lt;html lang=\"ja\" prefix=\"og:...</td>\n",
       "      <td>APIA -ã¢ãã¢- APIA ABOUT APIA ã¢ãã¢ã«ã...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ja</td>\n",
       "      <td>96398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               domain                                               HTML  \\\n",
       "695          arena.pl  <!DOCTYPE html><html lang=\"pl\"><head><link rel...   \n",
       "614  anzlitlovers.com  <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 T...   \n",
       "701       arikair.com  <!DOCTYPE html>\\n<html class=\"load-full-screen...   \n",
       "566     android8o.com  <!DOCTYPE html>\\r\\n<html lang=\"ru\">\\r\\n<head>\\...   \n",
       "628     apiajapan.com  <!DOCTYPE html>\\n\\t<html lang=\"ja\" prefix=\"og:...   \n",
       "\n",
       "                                                  Text  \\\n",
       "695  Arena.pl   Platforma zakupowa   Bezpieczne zak...   \n",
       "614  ANZ LitLovers LitBlog | For lovers of Australi...   \n",
       "701  Fly Arik Air - West-Africa's leading airline o...   \n",
       "566  Прошивки Android Oreo 8 для смартфонов и планш...   \n",
       "628  APIA -ã¢ãã¢- APIA ABOUT APIA ã¢ãã¢ã«ã...   \n",
       "\n",
       "                               found_phrases language      len  \n",
       "695        Delivery, Payment, Basket, E-shop       pl  1079239  \n",
       "614                        Delivery, Payment       en   241540  \n",
       "701  Terms and conditions, Delivery, Payment      NaN   102234  \n",
       "566                                   Basket       ru    98454  \n",
       "628                                      NaN       ja    96398  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[\"len\"]=data_1[\"Text\"].apply(len)\n",
    "data_1.sort_values(\"len\",ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1[(data_1[\"len\"] >= 1000) & (data_1[\"len\"] <= 100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(755, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1=data_1.drop(columns=[\"HTML\",\"len\"])\n",
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>Text</th>\n",
       "      <th>found_phrases</th>\n",
       "      <th>language</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101shop.com</td>\n",
       "      <td>å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>å åç è§£æ æ-2020å åç è§£æ æä è1⁄21⁄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001cartes.com</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "      <td>Add to cart, add to cart, Shipping Method, Pay...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001cuponesdedescuento.cl</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "      <td>Delivery, E-shop</td>\n",
       "      <td>es</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain  \\\n",
       "0               0101shop.com   \n",
       "6             1001cartes.com   \n",
       "7  1001cuponesdedescuento.cl   \n",
       "\n",
       "                                                Text  \\\n",
       "0  å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...   \n",
       "6  Faire-part de naissance, mariage, baptême - vo...   \n",
       "7  Cupones de descuento en Chile hasta 80% OFF en...   \n",
       "\n",
       "                                       found_phrases language  \\\n",
       "0                                                NaN      NaN   \n",
       "6  Add to cart, add to cart, Shipping Method, Pay...       fr   \n",
       "7                                   Delivery, E-shop       es   \n",
       "\n",
       "                                     normalized_text  \n",
       "0  å åç è§£æ æ-2020å åç è§£æ æä è1⁄21⁄...  \n",
       "6  Faire-part de naissance, mariage, baptême - vo...  \n",
       "7  Cupones de descuento en Chile hasta 80% OFF en...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):  # Ensure input is a string\n",
    "        return text\n",
    "    \n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Normalize Unicode (preserve foreign letters, remove accents)\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFKC', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # Remove control characters (except line breaks)\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', text)  \n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Apply normalization function\n",
    "data_1['normalized_text'] = data_1['Text'].astype(str).apply(normalize_text)\n",
    "\n",
    "# Display the result\n",
    "data_1.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>Text</th>\n",
       "      <th>found_phrases</th>\n",
       "      <th>language</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101shop.com</td>\n",
       "      <td>å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>å åç è§£æ æ-2020å åç è§£æ æä è1⁄21⁄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001cartes.com</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "      <td>Add to cart, add to cart, Shipping Method, Pay...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001cuponesdedescuento.cl</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "      <td>Delivery, E-shop</td>\n",
       "      <td>es</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100comments.com</td>\n",
       "      <td>Product Reviews, Free Samples, New Products @ ...</td>\n",
       "      <td>Privacy policy</td>\n",
       "      <td>en</td>\n",
       "      <td>Product Reviews, Free Samples, New Products @ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      domain  \\\n",
       "0               0101shop.com   \n",
       "6             1001cartes.com   \n",
       "7  1001cuponesdedescuento.cl   \n",
       "8            100comments.com   \n",
       "\n",
       "                                                Text  \\\n",
       "0  å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...   \n",
       "6  Faire-part de naissance, mariage, baptême - vo...   \n",
       "7  Cupones de descuento en Chile hasta 80% OFF en...   \n",
       "8  Product Reviews, Free Samples, New Products @ ...   \n",
       "\n",
       "                                       found_phrases language  \\\n",
       "0                                                NaN      NaN   \n",
       "6  Add to cart, add to cart, Shipping Method, Pay...       fr   \n",
       "7                                   Delivery, E-shop       es   \n",
       "8                                     Privacy policy       en   \n",
       "\n",
       "                                     normalized_text  \n",
       "0  å åç è§£æ æ-2020å åç è§£æ æä è1⁄21⁄...  \n",
       "6  Faire-part de naissance, mariage, baptême - vo...  \n",
       "7  Cupones de descuento en Chile hasta 80% OFF en...  \n",
       "8  Product Reviews, Free Samples, New Products @ ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_consecutive_duplicates(text):\n",
    "    return re.sub(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>Text</th>\n",
       "      <th>found_phrases</th>\n",
       "      <th>language</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101shop.com</td>\n",
       "      <td>å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>å åç è§£æ-2020å åç è§£æä è1⁄21⁄2-01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1001cartes.com</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "      <td>Add to cart, add to cart, Shipping Method, Pay...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Faire-part de naissance, mariage, baptême - vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1001cuponesdedescuento.cl</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "      <td>Delivery, E-shop</td>\n",
       "      <td>es</td>\n",
       "      <td>Cupones de descuento en Chile hasta 80% OFF en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100comments.com</td>\n",
       "      <td>Product Reviews, Free Samples, New Products @ ...</td>\n",
       "      <td>Privacy policy</td>\n",
       "      <td>en</td>\n",
       "      <td>Product Reviews, Free Samples, New Products @ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100sp.ru</td>\n",
       "      <td>100sp — интернет-магазин. Товары по выгодным ц...</td>\n",
       "      <td>Delivery</td>\n",
       "      <td>ru</td>\n",
       "      <td>100sp — интернет-магазин. Товары по выгодным ц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>baermate.com</td>\n",
       "      <td>BAER-MATE: Sabor, Saúde e Energia, 100% Natura...</td>\n",
       "      <td>Payment</td>\n",
       "      <td>pt</td>\n",
       "      <td>BAER-MATE: Sabor, Saúde e Energia, 100% Natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>bafa.de</td>\n",
       "      <td>BAFA - Startseite Springe direkt zu: Inhalt Ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>de</td>\n",
       "      <td>BAFA - Startseite Springe direkt zu: Inhalt Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>baileylineroad.com</td>\n",
       "      <td>Baileylineroad Skip to content COURSES Expand ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baileylineroad Skip to content COURSES Expand ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bakeryandsnacks.com</td>\n",
       "      <td>Bakery and snacks, cereal, cakes and pastries ...</td>\n",
       "      <td>Delivery, Shopping cart, E-shop</td>\n",
       "      <td>en</td>\n",
       "      <td>Bakery and snacks, cereal, cakes and pastries ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>bakkersonline.be</td>\n",
       "      <td>Bakkersonline - Online bestellen bij de bakker...</td>\n",
       "      <td>Delivery, Payment</td>\n",
       "      <td>nl</td>\n",
       "      <td>Bakkersonline - Online bestellen bij de bakker...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>755 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        domain  \\\n",
       "0                 0101shop.com   \n",
       "6               1001cartes.com   \n",
       "7    1001cuponesdedescuento.cl   \n",
       "8              100comments.com   \n",
       "9                     100sp.ru   \n",
       "..                         ...   \n",
       "992               baermate.com   \n",
       "993                    bafa.de   \n",
       "995         baileylineroad.com   \n",
       "998        bakeryandsnacks.com   \n",
       "999           bakkersonline.be   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    å åç ´è§£æ¸¸æ-2020å åç ´è§£æ¸¸æä¸è½...   \n",
       "6    Faire-part de naissance, mariage, baptême - vo...   \n",
       "7    Cupones de descuento en Chile hasta 80% OFF en...   \n",
       "8    Product Reviews, Free Samples, New Products @ ...   \n",
       "9    100sp — интернет-магазин. Товары по выгодным ц...   \n",
       "..                                                 ...   \n",
       "992  BAER-MATE: Sabor, Saúde e Energia, 100% Natura...   \n",
       "993  BAFA - Startseite Springe direkt zu: Inhalt Ha...   \n",
       "995  Baileylineroad Skip to content COURSES Expand ...   \n",
       "998  Bakery and snacks, cereal, cakes and pastries ...   \n",
       "999  Bakkersonline - Online bestellen bij de bakker...   \n",
       "\n",
       "                                         found_phrases language  \\\n",
       "0                                                  NaN      NaN   \n",
       "6    Add to cart, add to cart, Shipping Method, Pay...       fr   \n",
       "7                                     Delivery, E-shop       es   \n",
       "8                                       Privacy policy       en   \n",
       "9                                             Delivery       ru   \n",
       "..                                                 ...      ...   \n",
       "992                                            Payment       pt   \n",
       "993                                                NaN       de   \n",
       "995                                                NaN      NaN   \n",
       "998                    Delivery, Shopping cart, E-shop       en   \n",
       "999                                  Delivery, Payment       nl   \n",
       "\n",
       "                                       normalized_text  \n",
       "0    å åç è§£æ-2020å åç è§£æä è1⁄21⁄2-01...  \n",
       "6    Faire-part de naissance, mariage, baptême - vo...  \n",
       "7    Cupones de descuento en Chile hasta 80% OFF en...  \n",
       "8    Product Reviews, Free Samples, New Products @ ...  \n",
       "9    100sp — интернет-магазин. Товары по выгодным ц...  \n",
       "..                                                 ...  \n",
       "992  BAER-MATE: Sabor, Saúde e Energia, 100% Natura...  \n",
       "993  BAFA - Startseite Springe direkt zu: Inhalt Ha...  \n",
       "995  Baileylineroad Skip to content COURSES Expand ...  \n",
       "998  Bakery and snacks, cereal, cakes and pastries ...  \n",
       "999  Bakkersonline - Online bestellen bij de bakker...  \n",
       "\n",
       "[755 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['normalized_text']=data_1['normalized_text'].apply(remove_consecutive_duplicates)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_1[\"lang_len\"]=data_1[\"language\"].astype(str).apply(len)\n",
    "# data_1 = data_1[data_1[\"lang_len\"] == 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(755, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_last(text):\n",
    "    encoded_text = text\n",
    "\n",
    "    # Using UTF-8 encoding and ignoring errors\n",
    "    decoded_text = encoded_text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return decoded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"Decode\"]=data_1[\"normalized_text\"].apply(decode_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel(\"noshop_decoded.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the model once globally\n",
    "# nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "# nlp.add_pipe('sentencizer')\n",
    "\n",
    "# def Tokenizer(text):\n",
    "#     \"\"\"Tokenize a text into sentences using spaCy.\"\"\"\n",
    "#     if pd.isna(text) or not isinstance(text, str):  # Handle NaN or non-string values\n",
    "#         return []\n",
    "    \n",
    "#     doc = nlp(text)\n",
    "#     return [sent.text.strip() for sent in doc.sents]  # Return a list of sentences\n",
    "\n",
    "# # Apply the function to the 'text' column\n",
    "# data_1['tokenized_sentences'] = data_1['normalized_text'].apply(Tokenizer)\n",
    "\n",
    "# data_1.head(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"detected_language\"]=data_1[\"normalized_text\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "sum_mismatches = 0\n",
    "for i in range(len(data_1)):  \n",
    "    if data_1.iloc[i][\"language\"] != data_1.iloc[i][\"detected_language\"]:  \n",
    "        sum_mismatches += 1 \n",
    "print(sum_mismatches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_map = {\n",
    "    \"af\": \"afr_Latn\",\n",
    "    \"ak\": \"aka_Latn\",\n",
    "    \"am\": \"amh_Ethi\",\n",
    "    \"ar\": \"arb_Arab\",\n",
    "    \"as\": \"asm_Beng\",\n",
    "    \"ay\": \"ayr_Latn\",\n",
    "    \"az\": \"azj_Latn\",\n",
    "    \"bm\": \"bam_Latn\",\n",
    "    \"be\": \"bel_Cyrl\",\n",
    "    \"bn\": \"ben_Beng\",\n",
    "    \"bho\": \"bho_Deva\",\n",
    "    \"bs\": \"bos_Latn\",\n",
    "    \"bg\": \"bul_Cyrl\",\n",
    "    \"ca\": \"cat_Latn\",\n",
    "    \"ceb\": \"ceb_Latn\",\n",
    "    \"cs\": \"ces_Latn\",\n",
    "    \"ckb\": \"ckb_Arab\",\n",
    "    \"tt\": \"crh_Latn\",\n",
    "    \"cy\": \"cym_Latn\",\n",
    "    \"da\": \"dan_Latn\",\n",
    "    \"de\": \"deu_Latn\",\n",
    "    \"el\": \"ell_Grek\",\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"eo\": \"epo_Latn\",\n",
    "    \"et\": \"est_Latn\",\n",
    "    \"eu\": \"eus_Latn\",\n",
    "    \"ee\": \"ewe_Latn\",\n",
    "    \"fa\": \"pes_Arab\",\n",
    "    \"fi\": \"fin_Latn\",\n",
    "    \"fr\": \"fra_Latn\",\n",
    "    \"gd\": \"gla_Latn\",\n",
    "    \"ga\": \"gle_Latn\",\n",
    "    \"gl\": \"glg_Latn\",\n",
    "    \"gn\": \"grn_Latn\",\n",
    "    \"gu\": \"guj_Gujr\",\n",
    "    \"ht\": \"hat_Latn\",\n",
    "    \"ha\": \"hau_Latn\",\n",
    "    \"he\": \"heb_Hebr\",\n",
    "    \"hi\": \"hin_Deva\",\n",
    "    \"hr\": \"hrv_Latn\",\n",
    "    \"hu\": \"hun_Latn\",\n",
    "    \"hy\": \"hye_Armn\",\n",
    "    \"nl\": \"nld_Latn\",\n",
    "    \"ig\": \"ibo_Latn\",\n",
    "    \"ilo\": \"ilo_Latn\",\n",
    "    \"id\": \"ind_Latn\",\n",
    "    \"is\": \"isl_Latn\",\n",
    "    \"it\": \"ita_Latn\",\n",
    "    \"jv\": \"jav_Latn\",\n",
    "    \"ja\": \"jpn_Jpan\",\n",
    "    \"kn\": \"kan_Knda\",\n",
    "    \"ka\": \"kat_Geor\",\n",
    "    \"kk\": \"kaz_Cyrl\",\n",
    "    \"km\": \"khm_Khmr\",\n",
    "    \"rw\": \"kin_Latn\",\n",
    "    \"ko\": \"kor_Hang\",\n",
    "    \"ku\": \"kmr_Latn\",\n",
    "    \"lo\": \"lao_Laoo\",\n",
    "    \"lv\": \"lvs_Latn\",\n",
    "    \"ln\": \"lin_Latn\",\n",
    "    \"lt\": \"lit_Latn\",\n",
    "    \"lb\": \"ltz_Latn\",\n",
    "    \"lg\": \"lug_Latn\",\n",
    "    \"lus\": \"lus_Latn\",\n",
    "    \"mai\": \"mai_Deva\",\n",
    "    \"ml\": \"mal_Mlym\",\n",
    "    \"mr\": \"mar_Deva\",\n",
    "    \"mk\": \"mkd_Cyrl\",\n",
    "    \"mg\": \"plt_Latn\",\n",
    "    \"mt\": \"mlt_Latn\",\n",
    "    \"mni-Mtei\": \"mni_Beng\",\n",
    "    \"mni\": \"mni_Beng\",\n",
    "    \"mn\": \"khk_Cyrl\",\n",
    "    \"mi\": \"mri_Latn\",\n",
    "    \"ms\": \"zsm_Latn\",\n",
    "    \"my\": \"mya_Mymr\",\n",
    "    \"no\": \"nno_Latn\",\n",
    "    \"ne\": \"npi_Deva\",\n",
    "    \"ny\": \"nya_Latn\",\n",
    "    \"om\": \"gaz_Latn\",\n",
    "    \"or\": \"ory_Orya\",\n",
    "    \"pl\": \"pol_Latn\",\n",
    "    \"pt\": \"por_Latn\",\n",
    "    \"ps\": \"pbt_Arab\",\n",
    "    \"qu\": \"quy_Latn\",\n",
    "    \"ro\": \"ron_Latn\",\n",
    "    \"ru\": \"rus_Cyrl\",\n",
    "    \"sa\": \"san_Deva\",\n",
    "    \"si\": \"sin_Sinh\",\n",
    "    \"sk\": \"slk_Latn\",\n",
    "    \"sl\": \"slv_Latn\",\n",
    "    \"sm\": \"smo_Latn\",\n",
    "    \"sn\": \"sna_Latn\",\n",
    "    \"sd\": \"snd_Arab\",\n",
    "    \"so\": \"som_Latn\",\n",
    "    \"es\": \"spa_Latn\",\n",
    "    \"sq\": \"als_Latn\",\n",
    "    \"sr\": \"srp_Cyrl\",\n",
    "    \"su\": \"sun_Latn\",\n",
    "    \"sv\": \"swe_Latn\",\n",
    "    \"sw\": \"swh_Latn\",\n",
    "    \"ta\": \"tam_Taml\",\n",
    "    \"te\": \"tel_Telu\",\n",
    "    \"tg\": \"tgk_Cyrl\",\n",
    "    \"tl\": \"tgl_Latn\",\n",
    "    \"th\": \"tha_Thai\",\n",
    "    \"ti\": \"tir_Ethi\",\n",
    "    \"ts\": \"tso_Latn\",\n",
    "    \"tk\": \"tuk_Latn\",\n",
    "    \"tr\": \"tur_Latn\",\n",
    "    \"ug\": \"uig_Arab\",\n",
    "    \"uk\": \"ukr_Cyrl\",\n",
    "    \"ur\": \"urd_Arab\",\n",
    "    \"uz\": \"uzn_Latn\",\n",
    "    \"vi\": \"vie_Latn\",\n",
    "    \"xh\": \"xho_Latn\",\n",
    "    \"yi\": \"ydd_Hebr\",\n",
    "    \"yo\": \"yor_Latn\",\n",
    "    \"zh-CN\": \"zho_Hans\",\n",
    "    \"zh\": \"zho_Hans\",\n",
    "    \"zh-TW\": \"zho_Hant\",\n",
    "    \"zu\": \"zul_Latn\",\n",
    "    \"pa\": \"pan_Guru\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# import re\n",
    "\n",
    "# # Load NLLB model and tokenizer\n",
    "# model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# def chunk_text(text, chunk_size=1000):\n",
    "#     \"\"\"Splits text into chunks of specified size, preserving sentences where possible.\"\"\"\n",
    "#     sentences = re.split(r'(?<=[.!?]) +', text)  # Split while keeping punctuation-based structure\n",
    "#     chunks = []\n",
    "#     current_chunk = []\n",
    "#     current_length = 0\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         if current_length + len(sentence) + 1 > chunk_size:\n",
    "#             chunks.append(\" \".join(current_chunk))\n",
    "#             current_chunk = []\n",
    "#             current_length = 0\n",
    "#         current_chunk.append(sentence)\n",
    "#         current_length += len(sentence) + 1\n",
    "    \n",
    "#     if current_chunk:\n",
    "#         chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# def translate_chunks(text, src_lang, batch_size=4):\n",
    "#     \"\"\"Translates text efficiently by processing in batches, preserving non-text elements.\"\"\"\n",
    "#     if not text.strip():\n",
    "#         return \"[Empty content]\"\n",
    "    \n",
    "#     if src_lang == \"en\":\n",
    "#         return text  # Skip translation if language is English\n",
    "    \n",
    "#     src_lang_nllb = lang_map.get(src_lang, None)\n",
    "#     if not src_lang_nllb:\n",
    "#         return \"[Unsupported language]\"\n",
    "    \n",
    "#     chunks = chunk_text(text)\n",
    "#     translated_chunks = []\n",
    "    \n",
    "#     for i in range(0, len(chunks), batch_size):\n",
    "#         batch = chunks[i:i+batch_size]\n",
    "#         inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "#         translated_tokens = model.generate(\n",
    "#             **inputs,\n",
    "#             max_length=500,  # Increased limit for longer outputs\n",
    "#             num_beams=2,  # More beams for better quality\n",
    "#             forced_bos_token_id=tokenizer.convert_tokens_to_ids([\"eng_Latn\"])[0]\n",
    "#         )\n",
    "#         translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "#         translated_chunks.extend(translations)\n",
    "    \n",
    "#     return \" \".join(translated_chunks)\n",
    "\n",
    "# # Apply translation with batch processing, skipping English rows\n",
    "# data_1[\"translated_text\"] = data_1.apply(\n",
    "#     lambda row: row[\"normalized_text\"] if row[\"language\"] == \"en\" else translate_chunks(row[\"normalized_text\"], row[\"language\"]), axis=1\n",
    "# )\n",
    "\n",
    "# # Display final DataFrame\n",
    "# data_1.head(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful transaltion model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version I\n",
    "\n",
    "# from deep_translator import GoogleTranslator\n",
    "# import pandas as pd\n",
    "# import ast  # To safely convert string representation of lists\n",
    "\n",
    "# # Function to split long texts into chunks\n",
    "# def chunk_text(text, chunk_size=2000):\n",
    "#     return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# # Function to translate tokenized sentences\n",
    "# def translate_sentences(tokenized_list, src_lang=\"auto\", target_lang=\"en\"):\n",
    "#     if isinstance(tokenized_list, list):\n",
    "#         text = \" \".join(tokenized_list)  # Convert list to string\n",
    "#     else:\n",
    "#         text = str(tokenized_list)  # Ensure it's a string\n",
    "    \n",
    "#     # Split text into smaller chunks\n",
    "#     chunks = chunk_text(text)\n",
    "    \n",
    "#     # Translate each chunk separately\n",
    "#     translated_chunks = [\n",
    "#         GoogleTranslator(source=src_lang, target=target_lang).translate(chunk) for chunk in chunks\n",
    "#     ]\n",
    "    \n",
    "#     return \" \".join(translated_chunks)\n",
    "\n",
    "# # Apply translation conditionally\n",
    "# def apply_translation(row):\n",
    "#     if row[\"language\"] == \"en\":\n",
    "#         return row[\"tokenized_sentences\"]\n",
    "#     else:\n",
    "#         return translate_sentences(row[\"tokenized_sentences\"])\n",
    "\n",
    "# # Assuming data_1 is a Pandas DataFrame\n",
    "# data_1[\"translated_text_2\"] = data_1.apply(apply_translation, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet=data_1.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestError",
     "evalue": "Request exception can happen due to an api connection error. Please check your connection and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRequestError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(translated_chunks)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Apply translation to tokenized sentences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m snippet[\u001b[33m\"\u001b[39m\u001b[33mtranslated_text\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msnippet\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnormalized_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m snippet.head(\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtranslate_sentences\u001b[39m\u001b[34m(tokenized_list, src_lang, target_lang)\u001b[39m\n\u001b[32m     16\u001b[39m chunks = chunk_text(tokenized_list)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Translate each chunk separately\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m translated_chunks = \u001b[43m[\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Recombine the translated chunks into one string\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(translated_chunks)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     16\u001b[39m chunks = chunk_text(tokenized_list)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Translate each chunk separately\u001b[39;00m\n\u001b[32m     19\u001b[39m translated_chunks = [\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mGoogleTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Recombine the translated chunks into one string\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(translated_chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AniKhvadagiani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deep_translator\\google.py:74\u001b[39m, in \u001b[36mGoogleTranslator.translate\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests()\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_failed(status_code=response.status_code):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestError()\n\u001b[32m     76\u001b[39m soup = BeautifulSoup(response.text, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m element = soup.find(\u001b[38;5;28mself\u001b[39m._element_tag, \u001b[38;5;28mself\u001b[39m._element_query)\n",
      "\u001b[31mRequestError\u001b[39m: Request exception can happen due to an api connection error. Please check your connection and try again"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import pandas as pd\n",
    "import ast  # To safely convert string representation of lists\n",
    "\n",
    "\n",
    "# Function to split long texts into chunks\n",
    "def chunk_text(text, chunk_size=4000):\n",
    "    # Split the text into smaller chunks\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to translate tokenized sentences\n",
    "def translate_sentences(tokenized_list, src_lang=\"auto\", target_lang=\"en\"):\n",
    "   \n",
    "        \n",
    "        # Split text into smaller chunks\n",
    "        chunks = chunk_text(tokenized_list)\n",
    "        \n",
    "        # Translate each chunk separately\n",
    "        translated_chunks = [\n",
    "            GoogleTranslator(source=src_lang, target=target_lang).translate(chunk) for chunk in chunks\n",
    "        ]\n",
    "        \n",
    "        # Recombine the translated chunks into one string\n",
    "        return \" \".join(translated_chunks)\n",
    "    \n",
    "\n",
    "\n",
    "# Apply translation to tokenized sentences\n",
    "snippet[\"translated_text\"] = snippet[\"normalized_text\"].apply(translate_sentences)\n",
    "\n",
    "snippet.head(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_excel('test_3_transaltions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jfilter/clean-text/blob/main/README.md\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
