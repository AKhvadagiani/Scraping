{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef108710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e79ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1=pd.read_csv(\"Isshop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_payment_methods(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # All known payment keywords and how we want to report them\n",
    "        payment_keywords = {\n",
    "            'mastercard': 'mastercard',\n",
    "            'visa': 'visa',\n",
    "            'paypal': 'paypal',\n",
    "            'giropay': 'giropay',\n",
    "            'ideal': 'ideal',\n",
    "            'american express': 'amex',\n",
    "            'amex': 'amex',\n",
    "            'amazon pay': 'amazonpay',\n",
    "            'amazonpay': 'amazonpay',\n",
    "            'google pay': 'googlepay',\n",
    "            'googlepay': 'googlepay',\n",
    "            'maestro': 'maestro',\n",
    "            'alipay': 'alipay',\n",
    "            'wechat': 'wechatpay',\n",
    "            'wechat pay': 'wechatpay',\n",
    "            'unionpay': 'unionpay',\n",
    "            'jcb': 'jcb'\n",
    "        }\n",
    "\n",
    "        found = set()\n",
    "\n",
    "        # Find a relevant container (or fallback to whole doc)\n",
    "        container = (\n",
    "            soup.find('div', class_=lambda x: x and 'payment' in x.lower()) or\n",
    "            soup.find('section', class_=lambda x: x and 'payment' in x.lower()) or\n",
    "            soup\n",
    "        )\n",
    "\n",
    "        # 1. Look in image src and alt\n",
    "        for img in container.find_all('img'):\n",
    "            text = f\"{img.get('src', '')} {img.get('alt', '')}\".lower()\n",
    "            for keyword, label in payment_keywords.items():\n",
    "                if keyword in text:\n",
    "                    found.add(label)\n",
    "\n",
    "        # 2. Look in SVG title\n",
    "        for svg in container.find_all('svg'):\n",
    "            title = svg.find('title')\n",
    "            if title:\n",
    "                text = title.text.lower()\n",
    "                for keyword, label in payment_keywords.items():\n",
    "                    if keyword in text:\n",
    "                        found.add(label)\n",
    "\n",
    "        # 3. Look in class names\n",
    "        for tag in container.find_all(True):\n",
    "            class_list = tag.get('class', [])\n",
    "            for cls in class_list:\n",
    "                cls_lower = cls.lower()\n",
    "                for keyword, label in payment_keywords.items():\n",
    "                    if keyword.replace(' ', '') in cls_lower:  # remove spaces for class names\n",
    "                        found.add(label)\n",
    "\n",
    "        return ', '.join(list(sorted(found)))\n",
    "\n",
    "    except Exception:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f2a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1[\"payment_method\"]=data_1[\"HTML\"].apply(find_payment_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1892d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.to_csv(\"with_payment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a117fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shopbeachcity.com\n",
    "# wow-junkie.com\n",
    "# www.autos-erleben.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe49e45",
   "metadata": {},
   "source": [
    "logo detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d43a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 14.0778, Train Acc: 40.50%, Val Loss: 1.0041, Val Acc: 70.73%\n",
      "Epoch 2/20, Train Loss: 3.7047, Train Acc: 78.51%, Val Loss: 1.2934, Val Acc: 78.05%\n",
      "Epoch 3/20, Train Loss: 3.5242, Train Acc: 76.03%, Val Loss: 1.4092, Val Acc: 75.61%\n",
      "Epoch 4/20, Train Loss: 1.3485, Train Acc: 85.95%, Val Loss: 1.2900, Val Acc: 78.05%\n",
      "Early stopping triggered!\n",
      "Test Accuracy: 85.37%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_val_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_val_loss is None or val_loss < self.best_val_loss - self.min_delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_model_state = model.state_dict()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "\n",
    "# Path to your dataset\n",
    "data_dir = 'cards_dataset'\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # 3 channels\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Create indices and get labels\n",
    "indices = list(range(len(dataset)))\n",
    "labels = [label for _, label in dataset]\n",
    "\n",
    "# First split: train (60%) and temp (40%)\n",
    "train_idx, temp_idx = [], []\n",
    "for label in np.unique(labels):\n",
    "    label_idx = np.where(np.array(labels) == label)[0]\n",
    "    np.random.shuffle(label_idx)\n",
    "    split = int(0.6 * len(label_idx))\n",
    "    train_idx.extend(label_idx[:split])\n",
    "    temp_idx.extend(label_idx[split:])\n",
    "\n",
    "# Second split: val (50% of temp) and test (50% of temp)\n",
    "val_idx, test_idx = [], []\n",
    "for label in np.unique(labels):\n",
    "    label_idx = [i for i in temp_idx if labels[i] == label]\n",
    "    np.random.shuffle(label_idx)\n",
    "    split = int(0.5 * len(label_idx))\n",
    "    val_idx.extend(label_idx[:split])\n",
    "    test_idx.extend(label_idx[split:])\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# Create DataLoaders with samplers\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=2)\n",
    "\n",
    "\n",
    "# Define the improved CNN\n",
    "class LogoClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogoClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # Increased channels\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Additional conv layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Calculate correct flattened size: 128x128 -> 64x64 -> 32x32 -> 16x16\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)  # Increased size\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 128->64\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))   # 64->32\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))   # 32->16\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = LogoClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer with weight decay (L2 regularization)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)  # LR scheduler\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "\n",
    "# Training loop with validation\n",
    "num_epochs = 20  # Increased epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Train Acc: {100 * correct / total:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Val Acc: {100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "    # 🔁 Check early stopping\n",
    "    early_stopping(avg_val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Final test evaluation\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * test_correct / test_total:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'logo_classifier.pth')\n",
    "\n",
    "# Prediction function with proper image normalization\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "    return dataset.classes[predicted.item()], probabilities[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35691b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Others, Probabilities: [1.07444286e-07 9.99999881e-01 4.67454129e-08]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "class_name, probs = predict_image(\"test_visa.jpg\")\n",
    "print(f\"Prediction: {class_name}, Probabilities: {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b1e423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Distribution: {0: 30, 1: 61, 2: 30}\n",
      "Validation Distribution: {0: 10, 1: 21, 2: 10}\n",
      "Test Distribution: {0: 10, 1: 21, 2: 10}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to get class distribution for each split\n",
    "def get_class_distribution(loader, labels):\n",
    "    class_count = {i: 0 for i in np.unique(labels)}\n",
    "    for _, target in loader:\n",
    "        for label in target:\n",
    "            class_count[label.item()] += 1\n",
    "    return class_count\n",
    "\n",
    "# Get distributions for each split\n",
    "train_distribution = get_class_distribution(train_loader, labels)\n",
    "val_distribution = get_class_distribution(val_loader, labels)\n",
    "test_distribution = get_class_distribution(test_loader, labels)\n",
    "\n",
    "# Print distributions\n",
    "print(\"Train Distribution:\", train_distribution)\n",
    "print(\"Validation Distribution:\", val_distribution)\n",
    "print(\"Test Distribution:\", test_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Way to download first 50 random pics\n",
    "# # pip install duckduckgo-search --upgrade\n",
    "\n",
    "# import os\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "# from duckduckgo_search import DDGS  # Updated import\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Step 1: Search for images using the new DDGS() interface\n",
    "# results = DDGS().images(\"Visa Logo\", max_results=50)\n",
    "\n",
    "# # Step 2: Create folder if not exists\n",
    "# folder_path = r\"C:\\Users\\AniKhvadagiani\\Desktop\\For_Thesis\\Mastercard_dataset\\Visa\"\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# # Step 3: Download and save as PNG\n",
    "# for idx, result in enumerate(results):\n",
    "#     try:\n",
    "#         image_url = result[\"image\"]\n",
    "#         response = requests.get(image_url, timeout=10)\n",
    "#         image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "#         file_path = os.path.join(folder_path, f\"visa_{idx + 1}.png\")\n",
    "#         image.save(file_path, format=\"PNG\")\n",
    "#         print(f\"Saved: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to download image {idx + 1}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
